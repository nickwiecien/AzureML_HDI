{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08374ea9-c152-4042-9275-da15692f7810",
   "metadata": {},
   "source": [
    "# Connecting to HDInsight from Compute Instance\n",
    "Leveraging [JayDeBeAPI python package](https://github.com/baztian/jaydebeapi). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856c946-2b53-4511-a12a-c7b92bc1624b",
   "metadata": {},
   "source": [
    "### Install JDBC Driver and jaydebeapi package from PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084c7f6-e0a8-4f70-8d89-f62ea7802455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open terminal and execute following command prior to executing cell\n",
    "# pip install jaydebeapi\n",
    "\n",
    "!mkdir jdbcdriver\n",
    "!wget -O './jdbcdriver/hive-jdbc-uber-2.6.3.0-235.jar' https://github.com/timveil/hive-jdbc-uber-jar/releases/download/v1.8-2.6.3/hive-jdbc-uber-2.6.3.0-235.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad471f-4354-450d-9370-8e753aca9daf",
   "metadata": {},
   "source": [
    "### Connect to AML Workspace and retrieve pointer to default Azure Key Vault\n",
    "Retrieve HDI password from key vault. Note: this can be set using the syntax `kv.set_secret('hdipassword', 'XXXXXXXX')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1fbe2-32a5-4b98-a46a-b502d95a6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset, Model\n",
    "\n",
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get default key vault\n",
    "kv = ws.get_default_keyvault()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1a979-c7e0-43d2-8da5-d604ca432fcc",
   "metadata": {},
   "source": [
    "### Retrieve data from HDInsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f3f3a-b567-4e13-a0c2-5cf83602e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import pandas as pd\n",
    "\n",
    "driver = 'org.apache.hive.jdbc.HiveDriver'\n",
    "driver_path = './jdbcdriver/hive-jdbc-uber-2.6.3.0-235.jar'\n",
    "username = 'admin'\n",
    "password = kv.get_secret('hdipassword')\n",
    "server = kv.get_secret('hdiserver')\n",
    "\n",
    "conn = jaydebeapi.connect(driver,\n",
    "       f\"jdbc:hive2://{server}:443/;ssl=true;transportMode=http;httpPath=/hive2\", \n",
    "       [username, password],\n",
    "       driver_path)\n",
    "\n",
    "curs = conn.cursor()\n",
    "curs.execute('select * from hivesampletable limit 25')\n",
    "rows = curs.fetchall()   \n",
    "df = pd.DataFrame(rows, columns=[column[0].replace('hivesampletable.','') for column in curs.description])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23daf30-e9db-415a-9ff7-7ec197fe7c16",
   "metadata": {},
   "source": [
    "# Connect to HDInsight from Compute Cluster (AML Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59151def-2af8-472e-88dc-7a03d87fe8cf",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6d1de-cf5b-4ec2-80a1-3005932a9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2f0d8-07fd-4fef-9476-a6a2ac0ddf37",
   "metadata": {},
   "source": [
    "### Connect to AML workspace, create compute cluster, and retrieve pointer to default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e19d31-c54c-4d07-85a0-ca9504c8aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771baa1-b992-46cf-a859-89786a2a9342",
   "metadata": {},
   "source": [
    "### Define RunConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b4999-9d99-4ef4-93a8-f0d73a86fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = r\"\"\"FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
    "RUN mkdir -p /usr/share/man/man1\n",
    "RUN apt-get update && \\\n",
    "    DEBIAN_FRONTEND=noninteractive \\\n",
    "    apt-get -y install default-jre-headless && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "RUN mkdir jdbcdriver\n",
    "RUN wget -O './jdbcdriver/hive-jdbc-uber-2.6.3.0-235.jar' https://github.com/timveil/hive-jdbc-uber-jar/releases/download/v1.8-2.6.3/hive-jdbc-uber-2.6.3.0-235.jar\n",
    "ENV CLASSPATH=\"/jdbcdriver/hive-jdbc-uber-2.6.3.0-235.jar:${CLASSPATH}\"\n",
    "\"\"\"\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment(name='hdi_env')\n",
    "run_config.environment.docker.base_image = None\n",
    "run_config.environment.docker.base_dockerfile = dockerfile\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create()\n",
    "run_config.environment.python.conda_dependencies.set_pip_requirements([\n",
    "    'pandas==0.25.3',\n",
    "    'numpy==1.19.2',\n",
    "    'azureml-defaults==1.40.0',\n",
    "    'jaydebeapi==1.2.3'\n",
    "])\n",
    "run_config.environment.python.conda_dependencies.set_python_version('3.8.10')\n",
    "#Register environment for reuse \n",
    "run_config.environment.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4428a8-09a1-48d5-b641-7258e672dbd9",
   "metadata": {},
   "source": [
    "### Define Output Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c62f0c-9059-4eb8-80a5-c2771fc74f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_data = OutputFileDatasetConfig(name='HDI_Data', destination=(default_ds, 'hdi_data/{run-id}')).read_delimited_files().register_on_complete(name='HDI_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88a983-3bad-46ec-8b79-4c54cf49d141",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fb358-d7bc-4966-b97e-a9f3541a91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data from AML-linked datastore\n",
    "# Register tabular dataset after retrieval\n",
    "get_data_step = PythonScriptStep(\n",
    "    name='Get Data from Blob Storage',\n",
    "    script_name='query_hdinsight.py',\n",
    "    arguments =['--hdi_data', hdi_data],\n",
    "    outputs=[hdi_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bbde0-d407-4093-a642-e9b0c6047e62",
   "metadata": {},
   "source": [
    "### Create AML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dd012-3a6b-4f60-9e68-11d601108b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_data_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604ba89-1d7c-4ebf-9402-9b4c2079e5ee",
   "metadata": {},
   "source": [
    "### Submit Pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04e9d1-ebda-4451-9e01-e20e5ca5f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'hdi-query-testing')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
